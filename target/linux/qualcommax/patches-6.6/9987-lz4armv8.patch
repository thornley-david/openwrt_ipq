From be767c2beb07e51d1a9c0df6407f9860345b10b8 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?=C3=A9=C2=98=C2=BF=C3=A8=C2=8F=C2=8C=C3=A2=C2=80=C2=A2?=
 =?UTF-8?q?=C3=A6=C2=9C=C2=AA=C3=A9=C2=9C=C2=9C?= <799620521@qq.com>
Date: Tue, 2 Aug 2022 00:23:53 +0800
Subject: [PATCH] lib/lz4: Import arm64 V8 ASM lz4 decompression acceleration

This is a common cherry-pick on android kernle modding community.

Forward ported from 4.x to 6.6
Signed-off-by: backslashxx <118538522+backslashxx@users.noreply.github.com>
--- a/include/linux/lz4.h
+++ b/include/linux/lz4.h
@@ -644,5 +644,9 @@ int LZ4_decompress_safe_usingDict(const
  */
 int LZ4_decompress_fast_usingDict(const char *source, char *dest,
 	int originalSize, const char *dictStart, int dictSize);
+	
+ssize_t LZ4_arm64_decompress_safe(const void *source, void *dest, size_t inputSize, size_t outputSize, bool dip);
+
+ssize_t LZ4_arm64_decompress_safe_partial(const void *source, void *dest, size_t inputSize, size_t outputSize, bool dip);
 
 #endif
--- /dev/null
+++ b/lib/lz4/lz4armv8/lz4accel.c
@@ -0,0 +1,48 @@
+#include "lz4accel.h"
+#include <asm/cputype.h>
+
+#ifdef CONFIG_CFI_CLANG
+static inline int
+__cfi_lz4_decompress_asm(uint8_t **dst_ptr, uint8_t *dst_begin,
+			 uint8_t *dst_end, const uint8_t **src_ptr,
+			 const uint8_t *src_end, bool dip)
+{
+	return _lz4_decompress_asm(dst_ptr, dst_begin, dst_end,
+				   src_ptr, src_end, dip);
+}
+
+static inline int
+__cfi_lz4_decompress_asm_noprfm(uint8_t **dst_ptr, uint8_t *dst_begin,
+				uint8_t *dst_end, const uint8_t **src_ptr,
+				const uint8_t *src_end, bool dip)
+{
+	return _lz4_decompress_asm_noprfm(dst_ptr, dst_begin, dst_end,
+					  src_ptr, src_end, dip);
+}
+
+#define _lz4_decompress_asm		__cfi_lz4_decompress_asm
+#define _lz4_decompress_asm_noprfm	__cfi_lz4_decompress_asm_noprfm
+#endif
+
+int lz4_decompress_asm_select(uint8_t **dst_ptr, uint8_t *dst_begin,
+			      uint8_t *dst_end, const uint8_t **src_ptr,
+			      const uint8_t *src_end, bool dip) {
+	const unsigned i = smp_processor_id();
+
+	switch(read_cpuid_part_number()) {
+	//case ARM_CPU_PART_CORTEX_A53:
+		lz4_decompress_asm_fn[i] = _lz4_decompress_asm_noprfm;
+		return _lz4_decompress_asm_noprfm(dst_ptr, dst_begin, dst_end,
+						  src_ptr, src_end, dip);
+	}
+	lz4_decompress_asm_fn[i] = _lz4_decompress_asm;
+	return _lz4_decompress_asm(dst_ptr, dst_begin, dst_end,
+				   src_ptr, src_end, dip);
+}
+
+ int (*lz4_decompress_asm_fn[NR_CPUS])(uint8_t **dst_ptr, uint8_t *dst_begin,
+	uint8_t *dst_end, const uint8_t **src_ptr,
+	const uint8_t *src_end, bool dip)
+__read_mostly = {
+	[0 ... NR_CPUS-1]  = lz4_decompress_asm_select,
+};
--- /dev/null
+++ b/lib/lz4/lz4armv8/lz4accel.h
@@ -0,0 +1,56 @@
+#include <linux/types.h>
+#include <asm/simd.h>
+
+#define LZ4_FAST_MARGIN                (128)
+
+#if defined(CONFIG_ARM64)
+#include <asm/neon.h>
+#include <asm/cputype.h>
+
+asmlinkage int _lz4_decompress_asm(uint8_t **dst_ptr, uint8_t *dst_begin,
+				   uint8_t *dst_end, const uint8_t **src_ptr,
+				   const uint8_t *src_end, bool dip);
+
+asmlinkage int _lz4_decompress_asm_noprfm(uint8_t **dst_ptr, uint8_t *dst_begin,
+					  uint8_t *dst_end, const uint8_t **src_ptr,
+					  const uint8_t *src_end, bool dip);
+
+static inline int lz4_decompress_accel_enable(void)
+{
+	return	may_use_simd();
+}
+
+extern int (*lz4_decompress_asm_fn[])(uint8_t **dst_ptr, uint8_t *dst_begin,
+	uint8_t *dst_end, const uint8_t **src_ptr,
+	const uint8_t *src_end, bool dip);
+
+static inline ssize_t lz4_decompress_asm(
+	uint8_t **dst_ptr, uint8_t *dst_begin, uint8_t *dst_end,
+	const uint8_t **src_ptr, const uint8_t *src_end, bool dip)
+{
+	int ret;
+
+	kernel_neon_begin();
+	ret = lz4_decompress_asm_fn[smp_processor_id()](dst_ptr, dst_begin,
+							dst_end, src_ptr,
+							src_end, dip);
+	kernel_neon_end();
+	return (ssize_t)ret;
+}
+
+#define __ARCH_HAS_LZ4_ACCELERATOR
+
+#else
+
+static inline int lz4_decompress_accel_enable(void)
+{
+	return	0;
+}
+
+static inline ssize_t lz4_decompress_asm(
+	uint8_t **dst_ptr, uint8_t *dst_begin, uint8_t *dst_end,
+	const uint8_t **src_ptr, const uint8_t *src_end, bool dip)
+{
+	return 0;
+}
+#endif
--- /dev/null
+++ b/lib/lz4/lz4armv8/lz4armv8.S
@@ -0,0 +1,287 @@
+/*
+ * lz4armv8.S
+ * LZ4 decompression optimization based on arm64 NEON instruction
+ */
+
+#include <linux/linkage.h>
+#include <asm/assembler.h>
+
+#include <asm/kernel-pgtable.h>
+#include <asm/kexec.h>
+#include <asm/memory.h>
+#include <asm/page.h>
+/**
+ * _lz4_decompress_asm: The fast LZ4 decompression, lz4 decompression algothrim asm
+ * routine,support Huawei EROFS filesystem striving for maximum decompression speed.
+ * Entry point _lz4_decompress_asm.
+ * @para:
+ * x0 = current destination address ptr
+ * x1 = destination start position
+ * x2 = destination end position
+ * x3 = current source address ptr
+ * x4 = source end position
+ * x5 = flag for DIP
+ * @ret:
+ * 0 on success, -1 on failure
+ *
+ * x7: match_length
+ * x8: literal_legth
+ * x9: copy start ptr
+ * x10: copy end ptr
+ */
+
+
+#define match_length		x7
+#define literal_length		x8
+#define copy_from_ptr		x9    /* copy source ptr*/
+#define copy_to_ptr		x10   /* copy destination ptr*/
+#define w_tmp			w11   /* temp var */
+#define tmp			x11
+#define w_offset		w12
+#define offset			x12
+#define permtable_addr		x13
+#define cplen_table_addr	x14
+#define save_dst		x15
+#define save_src		x16
+#define offset_src_ptr		x17
+#define w_tmp_match_length	w18
+#define tmp_match_length	x18
+
+
+/* x3 >= x4 src overflow */
+.macro check_src_overflow
+	cmp	x3, x4
+	b.hs	Done
+.endm
+
+.macro check_src_overflow1
+	cmp	x3, x4
+	b.hs	Done1
+.endm
+/* x0 >= x2 dst overflow */
+.macro check_dst_overflow
+	cmp	x0, x2
+	b.hs	Done
+.endm
+
+.macro check_dst_overflow1
+	cmp	x0, x2
+	b.hs	Done1
+.endm
+
+.altmacro
+.macro lz4_decompress_asm_generic	doprfm=1
+	stp     x29, x30, [sp, #-16]!
+	mov     x29, sp
+	stp	x3, x0, [sp, #-16]!		/* push src and dst in stack */
+	ldr	x3, [x3]				/* x3 = *src_ptr */
+	ldr	x0, [x0]				/* x0 = *dst_ptr */
+	adr	permtable_addr, Permtable
+	adr	cplen_table_addr, Copylength_table
+
+	/*
+	 * save current dst and src ,ensure when return from asm routine
+	 * current both of "dst" and "src" save good position.
+	 */
+1: 	 mov save_dst, x0
+	mov	save_src, x3
+
+	check_dst_overflow
+	check_src_overflow
+
+.if \doprfm
+	add tmp, x0, #512
+	cmp x2, tmp
+	b.ls 2f
+	prfm pstl2strm,[x0,#512]
+.endif
+
+	/* Decode Token Byte: */
+2:	 ldrb w_tmp, [x3], #1		/* read Token Byte */
+	lsr	literal_length, tmp, #4	/* get literal_length */
+	and	tmp_match_length, tmp, #0xf	/* get match_length */
+	add	match_length, tmp_match_length, #4	/* match_length >=4 */
+
+	/*
+	 * literal_length <= 14 : no more literal length byte,fllowing zero
+	 * or more bytes are liteal bytes.
+	 */
+	cmp	literal_length, #14
+	b.ls	5f
+
+	/*
+	 * literal_length == 15 : more literal length bytes after TokenByte.
+	 * continue decoding more literal length bytes.
+	 */
+
+3: 	 check_src_overflow
+	ldrb	w_tmp, [x3], #1
+	add	literal_length, literal_length, tmp
+	cmp	tmp, #255
+	b.eq	3b
+
+/* literal copy */
+
+4:	 mov copy_from_ptr, x3
+	mov	copy_to_ptr, x0
+	add	x3, x3, literal_length
+	add	x0, x0, literal_length
+	check_dst_overflow
+	check_src_overflow
+
+4:	 ldr	q0, [copy_from_ptr], #16
+	str	q0, [copy_to_ptr], #16
+
+	cmp	x0, copy_to_ptr
+	b.ls	6f
+	b	4b
+
+5:	 ldr q0, [x3]
+	str q0, [x0]
+	add	x3, x3, literal_length
+	add	x0, x0, literal_length
+
+	/* Decode offset and match_length */
+6:	 mov	offset_src_ptr, x3
+	ldrh	w_offset, [x3], #2	/* 2Byte:offset bytes */
+	cbz	offset, Failed			/* match_length == 0 is invalid */
+	sub	copy_from_ptr, x0, offset
+	cmp	copy_from_ptr, x1
+	b.lo	Failed
+	mov	copy_to_ptr, x0
+	/*
+	 * set x0 to the end of "match copy";
+	 */
+	add	x0, x0, match_length
+	cmp	match_length, #19
+	b.lo	8f
+	/*
+	 * continue decoding more match length bytes.
+	 */
+
+7:	 check_src_overflow1
+	ldrb	w_tmp, [x3], #1
+	add	x0, x0, tmp
+	add	match_length, match_length, tmp
+	cmp	tmp, #255
+	b.eq	7b
+
+	/*
+	 * here got the matchlength,start "match copy".
+	 */
+
+8:	 check_dst_overflow1
+	cmp	offset , match_length
+	b.hs	13f
+
+9:	 cmp	offset , #32
+	b.hs	13f
+
+10:  ldr	q1, [copy_from_ptr]
+	add	tmp, permtable_addr, offset, lsl #5
+	ldp	q2, q3, [tmp]
+	tbl	v0.16b, {v1.16b}, v2.16b
+	tbl	v1.16b, {v1.16b}, v3.16b
+	cmp     offset , #16
+	b.lo    11f
+	ldp     q0, q1, [copy_from_ptr]
+
+11:	 ldrb	w_tmp, [cplen_table_addr, offset]
+	stp	q0, q1, [copy_to_ptr]
+	add	copy_to_ptr, copy_to_ptr, tmp
+	cmp	x0, copy_to_ptr
+	b.ls	1b
+
+12:  stp	q0, q1, [copy_to_ptr]
+	add	copy_to_ptr, copy_to_ptr, tmp
+	stp	q0, q1, [copy_to_ptr]
+	add	copy_to_ptr, copy_to_ptr, tmp
+	cmp	x0, copy_to_ptr
+	b.hi	12b
+	b	1b
+
+/* offset >= match */
+
+13:	 ldr	q0, [copy_from_ptr], #16
+	str	q0, [copy_to_ptr], #16
+
+	cmp	x0, copy_to_ptr
+	b.ls	1b
+
+14:  ldp	q0, q1, [copy_from_ptr], #32
+	stp	q0, q1, [copy_to_ptr], #32
+
+	cmp	x0, copy_to_ptr
+	b.hi	14b
+	b	1b
+.endm
+
+.text
+.p2align 4
+
+SYM_FUNC_START(_lz4_decompress_asm)
+	lz4_decompress_asm_generic
+SYM_FUNC_END(_lz4_decompress_asm)
+
+Failed:
+	mov	tmp, #-1
+	b	Exit_here
+
+Done1:
+	cbz	x5, Done
+	sub	save_src, offset_src_ptr, #1
+	strb	w_tmp_match_length, [save_src]
+	add	save_dst,save_dst,literal_length
+Done:
+	mov	tmp, #0
+
+Exit_here:
+	ldp	x3, x0, [sp], #16
+	str	save_src, [x3]
+	str	save_dst, [x0]
+	mov	x0, tmp
+	ldp     x29, x30, [sp], #16
+	ret     x30
+
+
+/*
+ * In case of offset <= 31 < matchlength ,expand the pattern and store in
+ * repeating pattern size(RPS),store the RPS in Copylength_table.
+ * case 1): 1 <= offset <= 15
+ * expand the pattern according to the Permtable and store their repeating pattern in q0 q1;
+ * RPS = 32 - (32 % offset) offset <= 31
+ * case 2): offset >= 16
+ * read the pattern and store in q0 q1.
+ * RPS = offset.
+ */
+.text
+.p2align 8
+Permtable:
+.byte 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0  //offset = 0
+.byte 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0  //offset = 1
+.byte 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1  //offset = 2
+.byte 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1  //offset = 3
+.byte 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3  //offset = 4
+.byte 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1  //offset = 5
+.byte 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1  //offset = 6
+.byte 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3  //offset = 7
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7  //offset = 8
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4  //offset = 9
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1  //offset = 10
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9  //offset = 11
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11, 0, 1, 2, 3, 4, 5, 6, 7  //offset = 12
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12, 0, 1, 2, 3, 4, 5  //offset = 13
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13, 0, 1, 2, 3  //offset = 14
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14, 0, 1  //offset = 15
+
+.p2align 8
+Copylength_table:
+.byte 32,32,32,30,32,30,30,28,32,27,30,22,24,26,28,30  // 0  .. 15
+.byte 16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31  // 16 .. 31
+
+
+.text
+.p2align 4
+SYM_FUNC_START(_lz4_decompress_asm_noprfm)
+	lz4_decompress_asm_generic	0
+SYM_FUNC_END(_lz4_decompress_asm_noprfm)
--- a/lib/lz4/lz4_decompress.c
+++ b/lib/lz4/lz4_decompress.c
@@ -40,6 +40,8 @@
 #include <linux/kernel.h>
 #include <asm/unaligned.h>
 
+#include "lz4armv8/lz4accel.h"
+
 /*-*****************************
  *	Decompression functions
  *******************************/
@@ -57,9 +59,11 @@
  * Note that it is important for performance that this function really get inlined,
  * in order to remove useless branches during compilation optimization.
  */
-static FORCE_INLINE int LZ4_decompress_generic(
+static FORCE_INLINE int __LZ4_decompress_generic(
 	 const char * const src,
 	 char * const dst,
+	 const BYTE * ip,
+	 BYTE * op,
 	 int srcSize,
 		/*
 		 * If endOnInput == endOnInputSize,
@@ -80,11 +84,9 @@ static FORCE_INLINE int LZ4_decompress_g
 	 const size_t dictSize
 	 )
 {
-	const BYTE *ip = (const BYTE *) src;
-	const BYTE * const iend = ip + srcSize;
+	const BYTE * const iend = src + srcSize;
 
-	BYTE *op = (BYTE *) dst;
-	BYTE * const oend = op + outputSize;
+	BYTE * const oend = dst + outputSize;
 	BYTE *cpy;
 
 	const BYTE * const dictEnd = (const BYTE *)dictStart + dictSize;
@@ -457,6 +459,33 @@ _output_error:
 	return (int) (-(((const char *)ip) - src)) - 1;
 }
 
+static FORCE_INLINE int LZ4_decompress_generic(
+	 const char * const src,
+	 char * const dst,
+	 int srcSize,
+		/*
+		 * If endOnInput == endOnInputSize,
+		 * this value is `dstCapacity`
+		 */
+	 int outputSize,
+	 /* endOnOutputSize, endOnInputSize */
+	 endCondition_directive endOnInput,
+	 /* full, partial */
+	 earlyEnd_directive partialDecoding,
+	 /* noDict, withPrefix64k, usingExtDict */
+	 dict_directive dict,
+	 /* always <= dst, == dst when no prefix */
+	 const BYTE * const lowPrefix,
+	 /* only if dict == usingExtDict */
+	 const BYTE * const dictStart,
+	 /* note : = 0 if noDict */
+	 const size_t dictSize
+	 )
+{
+	return __LZ4_decompress_generic(src, dst, (const BYTE *)src, (BYTE *)dst, srcSize, outputSize, endOnInput, partialDecoding, dict, lowPrefix, dictStart, dictSize);
+}
+
+
 int LZ4_decompress_safe(const char *source, char *dest,
 	int compressedSize, int maxDecompressedSize)
 {
@@ -705,6 +734,58 @@ int LZ4_decompress_fast_usingDict(const
 		dictStart, dictSize);
 }
 
+ssize_t LZ4_arm64_decompress_safe_partial(const void *source,
+			      void *dest,
+			      size_t inputSize,
+			      size_t outputSize,
+			      bool dip)
+{
+        uint8_t         *dstPtr = dest;
+        const uint8_t   *srcPtr = source;
+        ssize_t         ret;
+
+#ifdef __ARCH_HAS_LZ4_ACCELERATOR
+        /* Go fast if we can, keeping away from the end of buffers */
+        if (outputSize > LZ4_FAST_MARGIN && inputSize > LZ4_FAST_MARGIN && lz4_decompress_accel_enable()) {
+                ret = lz4_decompress_asm(&dstPtr, dest,
+                                         dest + outputSize - LZ4_FAST_MARGIN,
+                                         &srcPtr,
+                                         source + inputSize - LZ4_FAST_MARGIN,
+                                         dip);
+                if (ret)
+                        return -EIO;
+        }
+#endif
+        /* Finish in safe */
+	return __LZ4_decompress_generic(source, dest, srcPtr, dstPtr, inputSize, outputSize, endOnInputSize, partial_decode, noDict, (BYTE *)dest, NULL, 0);
+}
+
+ssize_t LZ4_arm64_decompress_safe(const void *source,
+			      void *dest,
+			      size_t inputSize,
+			      size_t outputSize,
+			      bool dip)
+{
+        uint8_t         *dstPtr = dest;
+        const uint8_t   *srcPtr = source;
+        ssize_t         ret;
+
+#ifdef __ARCH_HAS_LZ4_ACCELERATOR
+        /* Go fast if we can, keeping away from the end of buffers */
+        if (outputSize > LZ4_FAST_MARGIN && inputSize > LZ4_FAST_MARGIN && lz4_decompress_accel_enable()) {
+                ret = lz4_decompress_asm(&dstPtr, dest,
+                                         dest + outputSize - LZ4_FAST_MARGIN,
+                                         &srcPtr,
+                                         source + inputSize - LZ4_FAST_MARGIN,
+                                         dip);
+                if (ret)
+                        return -EIO;
+        }
+#endif
+        /* Finish in safe */
+	return __LZ4_decompress_generic(source, dest, srcPtr, dstPtr, inputSize, outputSize, endOnInputSize, decode_full_block, noDict, (BYTE *)dest, NULL, 0);
+}
+
 #ifndef STATIC
 EXPORT_SYMBOL(LZ4_decompress_safe);
 EXPORT_SYMBOL(LZ4_decompress_safe_partial);
@@ -715,6 +796,9 @@ EXPORT_SYMBOL(LZ4_decompress_fast_contin
 EXPORT_SYMBOL(LZ4_decompress_safe_usingDict);
 EXPORT_SYMBOL(LZ4_decompress_fast_usingDict);
 
+EXPORT_SYMBOL(LZ4_arm64_decompress_safe);
+EXPORT_SYMBOL(LZ4_arm64_decompress_safe_partial);
+
 MODULE_LICENSE("Dual BSD/GPL");
 MODULE_DESCRIPTION("LZ4 decompressor");
 #endif
--- a/lib/lz4/Makefile
+++ b/lib/lz4/Makefile
@@ -4,3 +4,5 @@ ccflags-y += -O3
 obj-$(CONFIG_LZ4_COMPRESS) += lz4_compress.o
 obj-$(CONFIG_LZ4HC_COMPRESS) += lz4hc_compress.o
 obj-$(CONFIG_LZ4_DECOMPRESS) += lz4_decompress.o
+
+obj-$(CONFIG_ARM64) += $(addprefix lz4armv8/, lz4accel.o lz4armv8.o)
